{
  "packId": "cv_cnn",
  "name": "CV/CNN Starter Pack",
  "categories": [
    {
      "id": "data",
      "name": "Data",
      "blocks": [
        {
          "blockId": "set_seed",
          "name": "Set seed / reproducibility",
          "category": "Data",
          "defaultCode": "import random\nimport numpy as np\nimport torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n",
          "defaultOutputBadge": "None"
        },
        {
          "blockId": "device_setup",
          "name": "Device setup (cuda/mps/cpu)",
          "category": "Data",
          "defaultCode": "import torch\n\ndef get_device() -> torch.device:\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    if torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    return torch.device(\"cpu\")\n\ndevice = get_device()\nprint(f\"Using device: {device}\")\n",
          "defaultOutputBadge": "torch.device"
        },
        {
          "blockId": "imagefolder_dataset",
          "name": "Torchvision ImageFolder dataset",
          "category": "Data",
          "defaultCode": "from torchvision import datasets\n\ntrain_dataset = datasets.ImageFolder(root=\"data/train\", transform=train_tf)\nval_dataset = datasets.ImageFolder(root=\"data/val\", transform=val_tf)\n",
          "defaultOutputBadge": "torchvision.datasets.ImageFolder"
        },
        {
          "blockId": "train_val_split",
          "name": "Train/val split",
          "category": "Data",
          "defaultCode": "from torch.utils.data import random_split\n\ntrain_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
          "defaultOutputBadge": "Tuple[Dataset, Dataset]"
        },
        {
          "blockId": "dataloaders",
          "name": "DataLoader (train/val)",
          "category": "Data",
          "defaultCode": "from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
          "defaultOutputBadge": "torch.utils.data.DataLoader"
        }
      ]
    },
    {
      "id": "transforms",
      "name": "Transforms",
      "blocks": [
        {
          "blockId": "resize_center_crop",
          "name": "Resize + CenterCrop + ToTensor + Normalize",
          "category": "Transforms",
          "defaultCode": "from torchvision import transforms as T\n\nimg_size = 224\ntrain_tf = T.Compose([\n    T.Resize((img_size, img_size)),\n    T.CenterCrop(img_size),\n    T.ToTensor(),\n    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n])\n\nval_tf = T.Compose([\n    T.Resize((img_size, img_size)),\n    T.CenterCrop(img_size),\n    T.ToTensor(),\n    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n])\n",
          "defaultOutputBadge": "torch.Tensor [B,3,H,W]"
        },
        {
          "blockId": "augmentations",
          "name": "Augmentations (RandomResizedCrop, HFlip, ColorJitter)",
          "category": "Transforms",
          "defaultCode": "from torchvision import transforms as T\n\nimg_size = 224\ntrain_tf = T.Compose([\n    T.RandomResizedCrop(img_size),\n    T.RandomHorizontalFlip(),\n    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    T.ToTensor(),\n    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n])\n",
          "defaultOutputBadge": "torch.Tensor [B,3,H,W]"
        },
        {
          "blockId": "custom_transforms",
          "name": "Custom transforms placeholder",
          "category": "Transforms",
          "defaultCode": "# TODO: define custom transforms\ntrain_tf = None\nval_tf = None\n",
          "defaultOutputBadge": "torch.Tensor [B,3,H,W]"
        }
      ]
    },
    {
      "id": "model",
      "name": "Model",
      "blocks": [
        {
          "blockId": "basic_cnn",
          "name": "Basic CNN (small)",
          "category": "Model",
          "defaultCode": "import torch\nfrom torch import nn\n\nclass BasicCNN(nn.Module):\n    def __init__(self, num_classes: int):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(32 * 56 * 56, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.classifier(x)\n\nmodel = BasicCNN(num_classes=10)\n",
          "defaultOutputBadge": "torch.nn.Module"
        },
        {
          "blockId": "deep_cnn",
          "name": "Stronger CNN (deeper + BN + Dropout)",
          "category": "Model",
          "defaultCode": "import torch\nfrom torch import nn\n\nclass StrongerCNN(nn.Module):\n    def __init__(self, num_classes: int):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(0.3),\n            nn.Linear(64 * 56 * 56, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.classifier(x)\n\nmodel = StrongerCNN(num_classes=10)\n",
          "defaultOutputBadge": "torch.nn.Module"
        },
        {
          "blockId": "resnet18",
          "name": "Transfer learning: ResNet18 pretrained",
          "category": "Model",
          "defaultCode": "import torch\nfrom torch import nn\nfrom torchvision import models\n\nmodel = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\nmodel.fc = nn.Linear(model.fc.in_features, 10)\n",
          "defaultOutputBadge": "torch.nn.Module"
        }
      ]
    },
    {
      "id": "training",
      "name": "Training",
      "blocks": [
        {
          "blockId": "loss_optimizer",
          "name": "Loss + optimizer (CrossEntropy + Adam)",
          "category": "Training",
          "defaultCode": "import torch\nfrom torch import nn\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
          "defaultOutputBadge": "Optimizer"
        },
        {
          "blockId": "scheduler",
          "name": "Scheduler (StepLR)",
          "category": "Training",
          "defaultCode": "from torch.optim.lr_scheduler import StepLR\n\nscheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
          "defaultOutputBadge": "LRScheduler"
        },
        {
          "blockId": "training_loop",
          "name": "Training loop (epoch/batch)",
          "category": "Training",
          "defaultCode": "for epoch in range(num_epochs):\n    model.train()\n    for batch in train_loader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n",
          "defaultOutputBadge": "None"
        },
        {
          "blockId": "validation_loop",
          "name": "Validation loop",
          "category": "Training",
          "defaultCode": "model.eval()\ncorrect = 0\nwith torch.no_grad():\n    for batch in val_loader:\n        inputs, targets = batch\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = model(inputs)\n        preds = outputs.argmax(dim=1)\n        correct += (preds == targets).sum().item()\n\nval_acc = correct / len(val_loader.dataset)\nprint(f\"Val acc: {val_acc:.4f}\")\n",
          "defaultOutputBadge": "float"
        },
        {
          "blockId": "save_checkpoint",
          "name": "Save checkpoints",
          "category": "Training",
          "defaultCode": "import torch\n\ntorch.save(model.state_dict(), \"checkpoint.pt\")\n",
          "defaultOutputBadge": "None"
        }
      ]
    },
    {
      "id": "metrics",
      "name": "Metrics/Debug",
      "blocks": [
        {
          "blockId": "accuracy_topk",
          "name": "Accuracy + top-k",
          "category": "Metrics",
          "defaultCode": "def accuracy_topk(outputs, targets, topk=(1,)):\n    maxk = max(topk)\n    _, pred = outputs.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n    res = []\n    for k in topk:\n        res.append(correct[:k].reshape(-1).float().sum(0, keepdim=True))\n    return res\n",
          "defaultOutputBadge": "list[float]"
        },
        {
          "blockId": "shape_debug",
          "name": "Print tensor shapes debug block",
          "category": "Metrics",
          "defaultCode": "print(\"inputs:\", inputs.shape)\nprint(\"outputs:\", outputs.shape)\n",
          "defaultOutputBadge": "None"
        }
      ]
    }
  ]
}

